{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'essay0',\n",
      "       'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7',\n",
      "       'essay8', 'essay9', 'ethnicity', 'height', 'income', 'job',\n",
      "       'last_online', 'location', 'offspring', 'orientation', 'pets',\n",
      "       'religion', 'sex', 'sign', 'smokes', 'speaks', 'status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a little extra', 'average', 'thin', 'athletic', 'fit', nan,\n",
       "       'skinny', 'curvy', 'full figured', 'jacked', 'rather not say',\n",
       "       'used up', 'overweight'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.body_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['body_type'] = df['body_type'].fillna('rather not say')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['strictly anything', 'mostly other', 'anything', 'vegetarian', nan,\n",
       "       'mostly anything', 'mostly vegetarian', 'strictly vegan',\n",
       "       'strictly vegetarian', 'mostly vegan', 'strictly other',\n",
       "       'mostly halal', 'other', 'vegan', 'mostly kosher',\n",
       "       'strictly halal', 'halal', 'strictly kosher', 'kosher'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diet.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diet'] = df['diet'].str.replace('strictly ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diet']= df['diet'].str.replace('mostly ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diet']= df['diet'].fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anything', 'other', 'vegetarian', 'vegan', 'halal', 'kosher'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diet.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['socially', 'often', 'not at all', 'rarely', nan, 'very often',\n",
       "       'desperately'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drinks.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['working on college/university', 'working on space camp',\n",
       "       'graduated from masters program',\n",
       "       'graduated from college/university', 'working on two-year college',\n",
       "       nan, 'graduated from high school', 'working on masters program',\n",
       "       'graduated from space camp', 'college/university',\n",
       "       'dropped out of space camp', 'graduated from ph.d program',\n",
       "       'graduated from law school', 'working on ph.d program',\n",
       "       'two-year college', 'graduated from two-year college',\n",
       "       'working on med school', 'dropped out of college/university',\n",
       "       'space camp', 'graduated from med school',\n",
       "       'dropped out of high school', 'working on high school',\n",
       "       'masters program', 'dropped out of ph.d program',\n",
       "       'dropped out of two-year college', 'dropped out of med school',\n",
       "       'high school', 'working on law school', 'law school',\n",
       "       'dropped out of masters program', 'ph.d program',\n",
       "       'dropped out of law school', 'med school'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.education.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education']= df['education'].str.replace('working on ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education']= df['education'].str.replace('graduated from ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education']= df['education'].str.replace('dropped out of ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['college/university', 'space camp', 'masters program',\n",
       "       'two-year college', nan, 'high school', 'ph.d program',\n",
       "       'law school', 'med school'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.education.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['south san francisco, california', 'oakland, california',\n",
       "       'san francisco, california', 'berkeley, california',\n",
       "       'belvedere tiburon, california', 'san mateo, california',\n",
       "       'daly city, california', 'san leandro, california',\n",
       "       'atherton, california', 'san rafael, california',\n",
       "       'walnut creek, california', 'menlo park, california',\n",
       "       'belmont, california', 'san jose, california',\n",
       "       'palo alto, california', 'emeryville, california',\n",
       "       'el granada, california', 'castro valley, california',\n",
       "       'fairfax, california', 'mountain view, california',\n",
       "       'burlingame, california', 'martinez, california',\n",
       "       'pleasant hill, california', 'hayward, california',\n",
       "       'alameda, california', 'vallejo, california',\n",
       "       'benicia, california', 'el cerrito, california',\n",
       "       'mill valley, california', 'richmond, california',\n",
       "       'redwood city, california', 'el sobrante, california',\n",
       "       'stanford, california', 'san pablo, california',\n",
       "       'novato, california', 'pacifica, california',\n",
       "       'lafayette, california', 'half moon bay, california',\n",
       "       'fremont, california', 'orinda, california',\n",
       "       'san anselmo, california', 'corte madera, california',\n",
       "       'albany, california', 'san carlos, california',\n",
       "       'san lorenzo, california', 'foster city, california',\n",
       "       'hercules, california', 'santa cruz, california',\n",
       "       'bolinas, california', 'sausalito, california',\n",
       "       'millbrae, california', 'larkspur, california',\n",
       "       'moraga, california', 'san bruno, california',\n",
       "       'petaluma, california', 'pinole, california',\n",
       "       'san geronimo, california', 'crockett, california',\n",
       "       'boulder, colorado', 'brisbane, california', 'freedom, california',\n",
       "       'montara, california', 'green brae, california',\n",
       "       'woodside, california', 'new york, new york', 'ross, california',\n",
       "       'east palo alto, california', 'san quentin, california',\n",
       "       'portland, oregon', 'rodeo, california',\n",
       "       'hacienda heights, california', 'woodacre, california',\n",
       "       'westlake, california', 'riverside, california',\n",
       "       'rohnert park, california', 'sacramento, california',\n",
       "       'point richmond, california', 'san diego, california',\n",
       "       'canyon country, california', 'tucson, arizona',\n",
       "       'honolulu, hawaii', 'billings, montana',\n",
       "       'west oakland, california', 'kentfield, california',\n",
       "       'milwaukee, wisconsin', 'woodbridge, virginia',\n",
       "       'glencove, california', 'tiburon, california', 'madrid, spain',\n",
       "       'las vegas, nevada', 'peoria, illinois',\n",
       "       'santa monica, california', 'bellwood, illinois',\n",
       "       'los angeles, california', 'moss beach, california',\n",
       "       'nha trang, vietnam', 'hillsborough, california',\n",
       "       'olema, california', 'union city, california', 'colma, california',\n",
       "       'cork, ireland', 'new orleans, louisiana',\n",
       "       'kensington, california', 'redwood shores, california',\n",
       "       'utica, michigan', 'brea, california', 'lagunitas, california',\n",
       "       'stinson beach, california', 'santa clara, california',\n",
       "       'studio city, california', 'concord, california',\n",
       "       'piedmont, california', 'grand rapids, michigan',\n",
       "       'seaside, california', 'leander, texas',\n",
       "       'forest knolls, california', 'edinburgh, united kingdom',\n",
       "       'magalia, california', 'london, united kingdom',\n",
       "       'astoria, new york', 'chicago, illinois', 'orange, california',\n",
       "       'south wellfleet, massachusetts', 'bayshore, california',\n",
       "       'asheville, north carolina', 'los gatos, california',\n",
       "       'boise, idaho', 'islip terrace, new york', 'sunnyvale, california',\n",
       "       'cambridge, massachusetts', 'lake orion, michigan',\n",
       "       'ozone park, new york', 'jackson, mississippi',\n",
       "       'ashland, california', 'south orange, new jersey',\n",
       "       'fort lauderdale, florida', 'minneapolis, minnesota',\n",
       "       'pasadena, california', 'atlanta, georgia', 'salt lake city, utah',\n",
       "       'arcadia, california', 'milpitas, california',\n",
       "       'san antonio, texas', 'port costa, california',\n",
       "       'nicasio, california', 'livingston, california',\n",
       "       'bellingham, washington', 'crowley, texas',\n",
       "       'boston, massachusetts', 'longwood, florida',\n",
       "       'fayetteville, west virginia', 'granite bay, california',\n",
       "       'isla vista, california', 'hilarita, california',\n",
       "       'campbell, california', 'stratford, connecticut',\n",
       "       'santa ana, california', 'santa rosa, california', 'kula, hawaii',\n",
       "       'murfreesboro, tennessee', 'brooklyn, new york',\n",
       "       'north hollywood, california', 'nevada city, california',\n",
       "       'providence, rhode island', 'stockton, california',\n",
       "       'marin city, california', 'washington, district of columbia',\n",
       "       'waterford, california', 'vancouver, british columbia, canada',\n",
       "       'muir beach, california', 'pacheco, california',\n",
       "       'irvine, california', 'kansas city, missouri', 'kassel, germany',\n",
       "       'canyon, california', 'philadelphia, pennsylvania',\n",
       "       'oceanview, california', 'long beach, new york',\n",
       "       'amsterdam, netherlands', 'taunton, massachusetts',\n",
       "       'napa, california', 'austin, texas', 'san luis obispo, california',\n",
       "       'modesto, california', 'bonaduz, switzerland',\n",
       "       'costa mesa, california', 'guadalajara, mexico',\n",
       "       'oakley, california', 'columbus, ohio', 'chico, california',\n",
       "       'south lake tahoe, california', 'vacaville, california',\n",
       "       'miami, florida', 'long beach, california', 'denver, colorado',\n",
       "       'seattle, washington', 'cincinnati, ohio', 'phoenix, arizona',\n",
       "       'rochester, michigan'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'] = df['location'].str.split(', ').str[-1]\n",
    "\n",
    "# Replace the 'location' column with the 'state' column\n",
    "df['location'] = df['state']\n",
    "\n",
    "# Drop the 'state' column\n",
    "df.drop(columns=['state'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['california', 'colorado', 'new york', 'oregon', 'arizona',\n",
       "       'hawaii', 'montana', 'wisconsin', 'virginia', 'spain', 'nevada',\n",
       "       'illinois', 'vietnam', 'ireland', 'louisiana', 'michigan', 'texas',\n",
       "       'united kingdom', 'massachusetts', 'north carolina', 'idaho',\n",
       "       'mississippi', 'new jersey', 'florida', 'minnesota', 'georgia',\n",
       "       'utah', 'washington', 'west virginia', 'connecticut', 'tennessee',\n",
       "       'rhode island', 'district of columbia', 'canada', 'missouri',\n",
       "       'germany', 'pennsylvania', 'netherlands', 'switzerland', 'mexico',\n",
       "       'ohio'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To work with the essay data, need more computing power to extract topics. Of course we could extract common words or find a faster workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.models import CoherenceModel\n",
    "# from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate all the essays into a single list\n",
    "# essays = df['essay0'].tolist() + df['essay1'].tolist() + df['essay2'].tolist() + df['essay3'].tolist() + df['essay4'].tolist() + df['essay5'].tolist() + df['essay6'].tolist() + df['essay7'].tolist() + df['essay8'].tolist() + df['essay9'].tolist()\n",
    "\n",
    "# # Tokenize the essays\n",
    "# tokens = [simple_preprocess(str(essay)) for essay in essays]\n",
    "\n",
    "# # Create a dictionary from the tokens\n",
    "# dictionary = Dictionary(tokens)\n",
    "\n",
    "# # Create a corpus from the dictionary and tokens\n",
    "# corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "# # Train the LDA model on the corpus\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                             id2word=dictionary,\n",
    "#                                             num_topics=10,\n",
    "#                                             random_state=100,\n",
    "#                                             update_every=1,\n",
    "#                                             chunksize=100,\n",
    "#                                             passes=10,\n",
    "#                                             alpha='auto',\n",
    "#                                             per_word_topics=True)\n",
    "\n",
    "# # Get the topics for each essay\n",
    "# essay_topics = []\n",
    "# for i in range(len(essays)):\n",
    "#     bow = dictionary.doc2bow(tokens[i])\n",
    "#     topic_scores = lda_model.get_document_topics(bow)\n",
    "#     topic_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "#     top_topic = lda_model.show_topic(topic_scores[0][0])\n",
    "#     essay_topics.append(top_topic[0][0])\n",
    "    \n",
    "# # Add the topics to the DataFrame as a new column\n",
    "# df['essay_topics'] = essay_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity',\n",
       "       'height', 'income', 'job', 'last_online', 'location', 'offspring',\n",
       "       'orientation', 'pets', 'religion', 'sex', 'sign', 'smokes', 'speaks',\n",
       "       'status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only columns containing the word \"essay\"\n",
    "cols_to_drop = df.filter(like='essay').columns\n",
    "\n",
    "# Drop the selected columns from the DataFrame\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     -1,   80000,   20000,   40000,   30000,   50000,   60000,\n",
       "       1000000,  150000,  100000,  500000,   70000,  250000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.income.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income']= df['income'].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,   80000,   20000,   40000,   30000,   50000,   60000,\n",
       "       1000000,  150000,  100000,  500000,   70000,  250000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.income.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['asian, white', 'white', nan, 'asian, black, other',\n",
       "       'white, other', 'hispanic / latin, white', 'hispanic / latin',\n",
       "       'pacific islander, white', 'asian', 'black, white',\n",
       "       'pacific islander', 'asian, native american',\n",
       "       'asian, pacific islander', 'black, native american, white',\n",
       "       'middle eastern, other', 'native american, white', 'indian',\n",
       "       'black', 'black, native american, hispanic / latin, other',\n",
       "       'black, native american, hispanic / latin',\n",
       "       'asian, black, pacific islander',\n",
       "       'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'other', 'hispanic / latin, other', 'asian, black',\n",
       "       'middle eastern, white', 'native american, white, other',\n",
       "       'black, native american', 'black, white, other',\n",
       "       'hispanic / latin, white, other', 'middle eastern', 'black, other',\n",
       "       'native american, hispanic / latin, white', 'black, indian',\n",
       "       'indian, white, other', 'middle eastern, indian, other',\n",
       "       'black, native american, hispanic / latin, white, other',\n",
       "       'pacific islander, hispanic / latin',\n",
       "       'black, hispanic / latin, white', 'native american',\n",
       "       'indian, white', 'asian, white, other', 'black, hispanic / latin',\n",
       "       'asian, hispanic / latin, white',\n",
       "       'middle eastern, hispanic / latin',\n",
       "       'asian, black, native american, pacific islander, white',\n",
       "       'middle eastern, indian', 'asian, indian',\n",
       "       'pacific islander, other', 'black, native american, white, other',\n",
       "       'black, pacific islander',\n",
       "       'middle eastern, native american, white',\n",
       "       'asian, native american, white, other',\n",
       "       'pacific islander, hispanic / latin, white', 'indian, other',\n",
       "       'asian, pacific islander, other', 'black, hispanic / latin, other',\n",
       "       'asian, black, native american',\n",
       "       'black, native american, hispanic / latin, white',\n",
       "       'native american, hispanic / latin', 'indian, hispanic / latin',\n",
       "       'native american, pacific islander',\n",
       "       'asian, black, native american, hispanic / latin, white',\n",
       "       'asian, black, white',\n",
       "       'asian, black, native american, pacific islander, other',\n",
       "       'middle eastern, hispanic / latin, white',\n",
       "       'asian, pacific islander, white',\n",
       "       'asian, native american, hispanic / latin, white, other',\n",
       "       'asian, hispanic / latin', 'asian, pacific islander, white, other',\n",
       "       'middle eastern, white, other',\n",
       "       'asian, pacific islander, hispanic / latin',\n",
       "       'black, native american, indian, other',\n",
       "       'native american, hispanic / latin, white, other',\n",
       "       'black, native american, other', 'asian, other',\n",
       "       'middle eastern, hispanic / latin, other',\n",
       "       'pacific islander, hispanic / latin, white, other',\n",
       "       'asian, black, hispanic / latin',\n",
       "       'asian, pacific islander, hispanic / latin, white',\n",
       "       'asian, black, native american, white',\n",
       "       'asian, middle eastern, white, other',\n",
       "       'native american, pacific islander, hispanic / latin',\n",
       "       'asian, native american, white',\n",
       "       'native american, pacific islander, hispanic / latin, white, other',\n",
       "       'indian, pacific islander', 'asian, middle eastern, black',\n",
       "       'asian, middle eastern, indian', 'asian, middle eastern, white',\n",
       "       'pacific islander, white, other',\n",
       "       'black, pacific islander, hispanic / latin',\n",
       "       'asian, middle eastern', 'asian, hispanic / latin, other',\n",
       "       'middle eastern, black, native american, indian, white, other',\n",
       "       'middle eastern, pacific islander, other', 'middle eastern, black',\n",
       "       'asian, indian, pacific islander',\n",
       "       'black, native american, pacific islander',\n",
       "       'native american, indian',\n",
       "       'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'black, indian, other',\n",
       "       'asian, middle eastern, indian, hispanic / latin, white, other',\n",
       "       'middle eastern, black, white',\n",
       "       'asian, hispanic / latin, white, other',\n",
       "       'native american, hispanic / latin, other',\n",
       "       'middle eastern, black, pacific islander, white',\n",
       "       'asian, black, native american, hispanic / latin',\n",
       "       'native american, other', 'black, indian, white',\n",
       "       'asian, native american, hispanic / latin, white',\n",
       "       'black, native american, indian, white',\n",
       "       'middle eastern, black, indian, pacific islander, hispanic / latin, white',\n",
       "       'middle eastern, hispanic / latin, white, other',\n",
       "       'asian, black, native american, other',\n",
       "       'native american, pacific islander, hispanic / latin, white',\n",
       "       'asian, indian, other',\n",
       "       'middle eastern, native american, hispanic / latin, white, other',\n",
       "       'asian, middle eastern, black, pacific islander, hispanic / latin, white',\n",
       "       'black, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'asian, middle eastern, native american, hispanic / latin, white',\n",
       "       'asian, middle eastern, black, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'asian, indian, white',\n",
       "       'native american, pacific islander, white, other',\n",
       "       'middle eastern, black, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'asian, middle eastern, other', 'middle eastern, pacific islander',\n",
       "       'asian, black, hispanic / latin, other',\n",
       "       'asian, middle eastern, black, native american, hispanic / latin, white',\n",
       "       'middle eastern, black, hispanic / latin',\n",
       "       'black, pacific islander, white',\n",
       "       'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, other',\n",
       "       'middle eastern, black, native american, indian, hispanic / latin, white',\n",
       "       'asian, pacific islander, hispanic / latin, white, other',\n",
       "       'middle eastern, indian, white', 'asian, indian, white, other',\n",
       "       'middle eastern, black, native american, white, other',\n",
       "       'black, native american, pacific islander, other',\n",
       "       'middle eastern, black, native american, white',\n",
       "       'asian, indian, pacific islander, other',\n",
       "       'asian, black, native american, white, other',\n",
       "       'black, indian, hispanic / latin, white',\n",
       "       'asian, middle eastern, black, native american, indian, pacific islander, white',\n",
       "       'asian, black, pacific islander, hispanic / latin',\n",
       "       'middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'asian, black, native american, indian',\n",
       "       'asian, black, indian, hispanic / latin, other',\n",
       "       'indian, hispanic / latin, other',\n",
       "       'asian, indian, hispanic / latin',\n",
       "       'asian, native american, pacific islander, white, other',\n",
       "       'asian, black, native american, indian, hispanic / latin, white, other',\n",
       "       'asian, indian, hispanic / latin, white',\n",
       "       'pacific islander, hispanic / latin, other',\n",
       "       'asian, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'indian, hispanic / latin, white',\n",
       "       'asian, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'asian, pacific islander, hispanic / latin, other',\n",
       "       'asian, black, hispanic / latin, white, other',\n",
       "       'black, indian, hispanic / latin',\n",
       "       'middle eastern, black, native american, hispanic / latin, white',\n",
       "       'black, pacific islander, other',\n",
       "       'black, native american, pacific islander, white',\n",
       "       'asian, black, native american, pacific islander',\n",
       "       'asian, indian, hispanic / latin, other',\n",
       "       'middle eastern, native american',\n",
       "       'middle eastern, native american, hispanic / latin',\n",
       "       'black, hispanic / latin, white, other',\n",
       "       'asian, native american, pacific islander, hispanic / latin, white',\n",
       "       'asian, native american, hispanic / latin',\n",
       "       'black, native american, indian, hispanic / latin, white, other',\n",
       "       'asian, middle eastern, hispanic / latin, white',\n",
       "       'black, native american, pacific islander, white, other',\n",
       "       'native american, indian, pacific islander, hispanic / latin',\n",
       "       'black, indian, white, other',\n",
       "       'asian, middle eastern, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'native american, pacific islander, white',\n",
       "       'middle eastern, indian, white, other',\n",
       "       'asian, black, white, other',\n",
       "       'middle eastern, native american, hispanic / latin, white',\n",
       "       'indian, hispanic / latin, white, other',\n",
       "       'asian, middle eastern, black, pacific islander',\n",
       "       'asian, middle eastern, black, indian, pacific islander, hispanic / latin, white',\n",
       "       'asian, middle eastern, indian, other',\n",
       "       'asian, middle eastern, black, white, other',\n",
       "       'black, native american, pacific islander, hispanic / latin, white',\n",
       "       'black, native american, indian, pacific islander, hispanic / latin',\n",
       "       'asian, black, pacific islander, white',\n",
       "       'middle eastern, pacific islander, hispanic / latin',\n",
       "       'black, native american, indian, white, other',\n",
       "       'asian, black, hispanic / latin, white',\n",
       "       'asian, black, native american, indian, pacific islander, white',\n",
       "       'asian, black, native american, indian, pacific islander, hispanic / latin',\n",
       "       'asian, middle eastern, hispanic / latin, white, other',\n",
       "       'middle eastern, black, native american, indian',\n",
       "       'asian, native american, pacific islander',\n",
       "       'asian, black, native american, pacific islander, white, other',\n",
       "       'asian, middle eastern, hispanic / latin',\n",
       "       'asian, black, pacific islander, other',\n",
       "       'asian, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'middle eastern, native american, white, other',\n",
       "       'asian, native american, hispanic / latin, other',\n",
       "       'native american, indian, white',\n",
       "       'black, native american, pacific islander, hispanic / latin',\n",
       "       'asian, native american, pacific islander, white',\n",
       "       'black, native american, indian',\n",
       "       'indian, pacific islander, hispanic / latin, white',\n",
       "       'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin',\n",
       "       'asian, middle eastern, indian, hispanic / latin',\n",
       "       'asian, middle eastern, native american, pacific islander, other',\n",
       "       'black, native american, indian, pacific islander',\n",
       "       'asian, middle eastern, native american, pacific islander, white, other',\n",
       "       'asian, native american, other', 'middle eastern, black, other',\n",
       "       'asian, black, pacific islander, hispanic / latin, white',\n",
       "       'asian, middle eastern, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'asian, native american, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'asian, middle eastern, black, pacific islander, hispanic / latin',\n",
       "       'asian, black, pacific islander, white, other',\n",
       "       'asian, black, indian'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ethnicity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['single', 'available', 'seeing someone', 'married', 'unknown'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity',\n",
       "       'height', 'income', 'job', 'last_online', 'location', 'offspring',\n",
       "       'orientation', 'pets', 'religion', 'sex', 'sign', 'smokes', 'speaks',\n",
       "       'status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['doesn&rsquo;t have kids, but might want them', nan,\n",
       "       'doesn&rsquo;t want kids',\n",
       "       'doesn&rsquo;t have kids, but wants them',\n",
       "       'doesn&rsquo;t have kids', 'wants kids', 'has a kid', 'has kids',\n",
       "       'doesn&rsquo;t have kids, and doesn&rsquo;t want any',\n",
       "       'has kids, but doesn&rsquo;t want more',\n",
       "       'has a kid, but doesn&rsquo;t want more',\n",
       "       'has a kid, and wants more', 'has kids, and might want more',\n",
       "       'might want kids', 'has a kid, and might want more',\n",
       "       'has kids, and wants more'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.offspring.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['offspring']= df['offspring'].str.replace('&rsquo;', \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"doesn't have kids, but might want them\", nan, \"doesn't want kids\",\n",
       "       \"doesn't have kids, but wants them\", \"doesn't have kids\",\n",
       "       'wants kids', 'has a kid', 'has kids',\n",
       "       \"doesn't have kids, and doesn't want any\",\n",
       "       \"has kids, but doesn't want more\",\n",
       "       \"has a kid, but doesn't want more\", 'has a kid, and wants more',\n",
       "       'has kids, and might want more', 'might want kids',\n",
       "       'has a kid, and might want more', 'has kids, and wants more'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.offspring.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['offspring']= df['offspring'].str.replace('a kid', 'kids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"doesn't have kids, but might want them\", nan, \"doesn't want kids\",\n",
       "       \"doesn't have kids, but wants them\", \"doesn't have kids\",\n",
       "       'wants kids', 'has kids',\n",
       "       \"doesn't have kids, and doesn't want any\",\n",
       "       \"has kids, but doesn't want more\", 'has kids, and wants more',\n",
       "       'has kids, and might want more', 'might want kids'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.offspring.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['height', 'last_online', 'sign', 'speaks', 'pets'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity',\n",
       "       'income', 'job', 'location', 'offspring', 'orientation', 'religion',\n",
       "       'sex', 'smokes', 'status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['transportation', 'hospitality / travel', nan, 'student',\n",
       "       'artistic / musical / writer', 'computer / hardware / software',\n",
       "       'banking / financial / real estate', 'entertainment / media',\n",
       "       'sales / marketing / biz dev', 'other', 'medicine / health',\n",
       "       'science / tech / engineering', 'executive / management',\n",
       "       'education / academia', 'clerical / administrative',\n",
       "       'construction / craftsmanship', 'rather not say',\n",
       "       'political / government', 'law / legal services', 'unemployed',\n",
       "       'military', 'retired'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.job.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split the dataframe into input features (X) and target variable (y)\n",
    "X = df.drop('orientation', axis=1)\n",
    "y = df['orientation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column transformer to one-hot encode categorical columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), ['body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity', 'job', 'location', 'offspring', 'religion', 'sex', 'smokes', 'status'])],\n",
    "                       remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the column transformer to the input features (X)\n",
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a decision tree classifier on the transformed data\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 0.7785654712260217\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the classifier on the test data\n",
    "score = clf.score(X_test, y_test)\n",
    "print(f\"Classifier accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['body_type_a little extra', 'body_type_athletic',\n",
       "       'body_type_average', 'body_type_curvy', 'body_type_fit',\n",
       "       'body_type_full figured', 'body_type_jacked',\n",
       "       'body_type_overweight', 'body_type_rather not say',\n",
       "       'body_type_skinny', 'body_type_thin', 'body_type_used up',\n",
       "       'diet_anything', 'diet_halal', 'diet_kosher', 'diet_other',\n",
       "       'diet_vegan', 'diet_vegetarian', 'drinks_desperately',\n",
       "       'drinks_not at all', 'drinks_often', 'drinks_rarely',\n",
       "       'drinks_socially', 'drinks_very often', 'drinks_nan',\n",
       "       'drugs_never', 'drugs_often', 'drugs_sometimes', 'drugs_nan',\n",
       "       'education_college/university', 'education_high school',\n",
       "       'education_law school', 'education_masters program',\n",
       "       'education_med school', 'education_ph.d program',\n",
       "       'education_space camp', 'education_two-year college',\n",
       "       'education_nan', 'ethnicity_asian', 'ethnicity_asian, black',\n",
       "       'ethnicity_asian, black, hispanic / latin',\n",
       "       'ethnicity_asian, black, hispanic / latin, other',\n",
       "       'ethnicity_asian, black, hispanic / latin, white',\n",
       "       'ethnicity_asian, black, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, black, indian',\n",
       "       'ethnicity_asian, black, indian, hispanic / latin, other',\n",
       "       'ethnicity_asian, black, native american',\n",
       "       'ethnicity_asian, black, native american, hispanic / latin',\n",
       "       'ethnicity_asian, black, native american, hispanic / latin, white',\n",
       "       'ethnicity_asian, black, native american, indian',\n",
       "       'ethnicity_asian, black, native american, indian, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, black, native american, indian, pacific islander, hispanic / latin',\n",
       "       'ethnicity_asian, black, native american, indian, pacific islander, white',\n",
       "       'ethnicity_asian, black, native american, other',\n",
       "       'ethnicity_asian, black, native american, pacific islander',\n",
       "       'ethnicity_asian, black, native american, pacific islander, other',\n",
       "       'ethnicity_asian, black, native american, pacific islander, white',\n",
       "       'ethnicity_asian, black, native american, pacific islander, white, other',\n",
       "       'ethnicity_asian, black, native american, white',\n",
       "       'ethnicity_asian, black, native american, white, other',\n",
       "       'ethnicity_asian, black, other',\n",
       "       'ethnicity_asian, black, pacific islander',\n",
       "       'ethnicity_asian, black, pacific islander, hispanic / latin',\n",
       "       'ethnicity_asian, black, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, black, pacific islander, other',\n",
       "       'ethnicity_asian, black, pacific islander, white',\n",
       "       'ethnicity_asian, black, pacific islander, white, other',\n",
       "       'ethnicity_asian, black, white',\n",
       "       'ethnicity_asian, black, white, other',\n",
       "       'ethnicity_asian, hispanic / latin',\n",
       "       'ethnicity_asian, hispanic / latin, other',\n",
       "       'ethnicity_asian, hispanic / latin, white',\n",
       "       'ethnicity_asian, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, indian',\n",
       "       'ethnicity_asian, indian, hispanic / latin',\n",
       "       'ethnicity_asian, indian, hispanic / latin, other',\n",
       "       'ethnicity_asian, indian, hispanic / latin, white',\n",
       "       'ethnicity_asian, indian, other',\n",
       "       'ethnicity_asian, indian, pacific islander',\n",
       "       'ethnicity_asian, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, indian, pacific islander, other',\n",
       "       'ethnicity_asian, indian, white',\n",
       "       'ethnicity_asian, indian, white, other',\n",
       "       'ethnicity_asian, middle eastern',\n",
       "       'ethnicity_asian, middle eastern, black',\n",
       "       'ethnicity_asian, middle eastern, black, indian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, middle eastern, black, native american, hispanic / latin, white',\n",
       "       'ethnicity_asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin',\n",
       "       'ethnicity_asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, other',\n",
       "       'ethnicity_asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, middle eastern, black, native american, indian, pacific islander, white',\n",
       "       'ethnicity_asian, middle eastern, black, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, middle eastern, black, pacific islander',\n",
       "       'ethnicity_asian, middle eastern, black, pacific islander, hispanic / latin',\n",
       "       'ethnicity_asian, middle eastern, black, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, middle eastern, black, white, other',\n",
       "       'ethnicity_asian, middle eastern, hispanic / latin',\n",
       "       'ethnicity_asian, middle eastern, hispanic / latin, white',\n",
       "       'ethnicity_asian, middle eastern, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, middle eastern, indian',\n",
       "       'ethnicity_asian, middle eastern, indian, hispanic / latin',\n",
       "       'ethnicity_asian, middle eastern, indian, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, middle eastern, indian, other',\n",
       "       'ethnicity_asian, middle eastern, native american, hispanic / latin, white',\n",
       "       'ethnicity_asian, middle eastern, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, middle eastern, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, middle eastern, native american, pacific islander, other',\n",
       "       'ethnicity_asian, middle eastern, native american, pacific islander, white, other',\n",
       "       'ethnicity_asian, middle eastern, other',\n",
       "       'ethnicity_asian, middle eastern, white',\n",
       "       'ethnicity_asian, middle eastern, white, other',\n",
       "       'ethnicity_asian, native american',\n",
       "       'ethnicity_asian, native american, hispanic / latin',\n",
       "       'ethnicity_asian, native american, hispanic / latin, other',\n",
       "       'ethnicity_asian, native american, hispanic / latin, white',\n",
       "       'ethnicity_asian, native american, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, native american, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, native american, other',\n",
       "       'ethnicity_asian, native american, pacific islander',\n",
       "       'ethnicity_asian, native american, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, native american, pacific islander, white',\n",
       "       'ethnicity_asian, native american, pacific islander, white, other',\n",
       "       'ethnicity_asian, native american, white',\n",
       "       'ethnicity_asian, native american, white, other',\n",
       "       'ethnicity_asian, other', 'ethnicity_asian, pacific islander',\n",
       "       'ethnicity_asian, pacific islander, hispanic / latin',\n",
       "       'ethnicity_asian, pacific islander, hispanic / latin, other',\n",
       "       'ethnicity_asian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_asian, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_asian, pacific islander, other',\n",
       "       'ethnicity_asian, pacific islander, white',\n",
       "       'ethnicity_asian, pacific islander, white, other',\n",
       "       'ethnicity_asian, white', 'ethnicity_asian, white, other',\n",
       "       'ethnicity_black', 'ethnicity_black, hispanic / latin',\n",
       "       'ethnicity_black, hispanic / latin, other',\n",
       "       'ethnicity_black, hispanic / latin, white',\n",
       "       'ethnicity_black, hispanic / latin, white, other',\n",
       "       'ethnicity_black, indian',\n",
       "       'ethnicity_black, indian, hispanic / latin',\n",
       "       'ethnicity_black, indian, hispanic / latin, white',\n",
       "       'ethnicity_black, indian, other', 'ethnicity_black, indian, white',\n",
       "       'ethnicity_black, indian, white, other',\n",
       "       'ethnicity_black, native american',\n",
       "       'ethnicity_black, native american, hispanic / latin',\n",
       "       'ethnicity_black, native american, hispanic / latin, other',\n",
       "       'ethnicity_black, native american, hispanic / latin, white',\n",
       "       'ethnicity_black, native american, hispanic / latin, white, other',\n",
       "       'ethnicity_black, native american, indian',\n",
       "       'ethnicity_black, native american, indian, hispanic / latin, white, other',\n",
       "       'ethnicity_black, native american, indian, other',\n",
       "       'ethnicity_black, native american, indian, pacific islander',\n",
       "       'ethnicity_black, native american, indian, pacific islander, hispanic / latin',\n",
       "       'ethnicity_black, native american, indian, white',\n",
       "       'ethnicity_black, native american, indian, white, other',\n",
       "       'ethnicity_black, native american, other',\n",
       "       'ethnicity_black, native american, pacific islander',\n",
       "       'ethnicity_black, native american, pacific islander, hispanic / latin',\n",
       "       'ethnicity_black, native american, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_black, native american, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_black, native american, pacific islander, other',\n",
       "       'ethnicity_black, native american, pacific islander, white',\n",
       "       'ethnicity_black, native american, pacific islander, white, other',\n",
       "       'ethnicity_black, native american, white',\n",
       "       'ethnicity_black, native american, white, other',\n",
       "       'ethnicity_black, other', 'ethnicity_black, pacific islander',\n",
       "       'ethnicity_black, pacific islander, hispanic / latin',\n",
       "       'ethnicity_black, pacific islander, other',\n",
       "       'ethnicity_black, pacific islander, white',\n",
       "       'ethnicity_black, white', 'ethnicity_black, white, other',\n",
       "       'ethnicity_hispanic / latin', 'ethnicity_hispanic / latin, other',\n",
       "       'ethnicity_hispanic / latin, white',\n",
       "       'ethnicity_hispanic / latin, white, other', 'ethnicity_indian',\n",
       "       'ethnicity_indian, hispanic / latin',\n",
       "       'ethnicity_indian, hispanic / latin, other',\n",
       "       'ethnicity_indian, hispanic / latin, white',\n",
       "       'ethnicity_indian, hispanic / latin, white, other',\n",
       "       'ethnicity_indian, other', 'ethnicity_indian, pacific islander',\n",
       "       'ethnicity_indian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_indian, white', 'ethnicity_indian, white, other',\n",
       "       'ethnicity_middle eastern', 'ethnicity_middle eastern, black',\n",
       "       'ethnicity_middle eastern, black, hispanic / latin',\n",
       "       'ethnicity_middle eastern, black, indian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_middle eastern, black, native american, hispanic / latin, white',\n",
       "       'ethnicity_middle eastern, black, native american, indian',\n",
       "       'ethnicity_middle eastern, black, native american, indian, hispanic / latin, white',\n",
       "       'ethnicity_middle eastern, black, native american, indian, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_middle eastern, black, native american, indian, white, other',\n",
       "       'ethnicity_middle eastern, black, native american, white',\n",
       "       'ethnicity_middle eastern, black, native american, white, other',\n",
       "       'ethnicity_middle eastern, black, other',\n",
       "       'ethnicity_middle eastern, black, pacific islander, white',\n",
       "       'ethnicity_middle eastern, black, white',\n",
       "       'ethnicity_middle eastern, hispanic / latin',\n",
       "       'ethnicity_middle eastern, hispanic / latin, other',\n",
       "       'ethnicity_middle eastern, hispanic / latin, white',\n",
       "       'ethnicity_middle eastern, hispanic / latin, white, other',\n",
       "       'ethnicity_middle eastern, indian',\n",
       "       'ethnicity_middle eastern, indian, other',\n",
       "       'ethnicity_middle eastern, indian, white',\n",
       "       'ethnicity_middle eastern, indian, white, other',\n",
       "       'ethnicity_middle eastern, native american',\n",
       "       'ethnicity_middle eastern, native american, hispanic / latin',\n",
       "       'ethnicity_middle eastern, native american, hispanic / latin, white',\n",
       "       'ethnicity_middle eastern, native american, hispanic / latin, white, other',\n",
       "       'ethnicity_middle eastern, native american, white',\n",
       "       'ethnicity_middle eastern, native american, white, other',\n",
       "       'ethnicity_middle eastern, other',\n",
       "       'ethnicity_middle eastern, pacific islander',\n",
       "       'ethnicity_middle eastern, pacific islander, hispanic / latin',\n",
       "       'ethnicity_middle eastern, pacific islander, other',\n",
       "       'ethnicity_middle eastern, white',\n",
       "       'ethnicity_middle eastern, white, other',\n",
       "       'ethnicity_native american',\n",
       "       'ethnicity_native american, hispanic / latin',\n",
       "       'ethnicity_native american, hispanic / latin, other',\n",
       "       'ethnicity_native american, hispanic / latin, white',\n",
       "       'ethnicity_native american, hispanic / latin, white, other',\n",
       "       'ethnicity_native american, indian',\n",
       "       'ethnicity_native american, indian, pacific islander, hispanic / latin',\n",
       "       'ethnicity_native american, indian, white',\n",
       "       'ethnicity_native american, other',\n",
       "       'ethnicity_native american, pacific islander',\n",
       "       'ethnicity_native american, pacific islander, hispanic / latin',\n",
       "       'ethnicity_native american, pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_native american, pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_native american, pacific islander, white',\n",
       "       'ethnicity_native american, pacific islander, white, other',\n",
       "       'ethnicity_native american, white',\n",
       "       'ethnicity_native american, white, other', 'ethnicity_other',\n",
       "       'ethnicity_pacific islander',\n",
       "       'ethnicity_pacific islander, hispanic / latin',\n",
       "       'ethnicity_pacific islander, hispanic / latin, other',\n",
       "       'ethnicity_pacific islander, hispanic / latin, white',\n",
       "       'ethnicity_pacific islander, hispanic / latin, white, other',\n",
       "       'ethnicity_pacific islander, other',\n",
       "       'ethnicity_pacific islander, white',\n",
       "       'ethnicity_pacific islander, white, other', 'ethnicity_white',\n",
       "       'ethnicity_white, other', 'ethnicity_nan',\n",
       "       'job_artistic / musical / writer',\n",
       "       'job_banking / financial / real estate',\n",
       "       'job_clerical / administrative',\n",
       "       'job_computer / hardware / software',\n",
       "       'job_construction / craftsmanship', 'job_education / academia',\n",
       "       'job_entertainment / media', 'job_executive / management',\n",
       "       'job_hospitality / travel', 'job_law / legal services',\n",
       "       'job_medicine / health', 'job_military', 'job_other',\n",
       "       'job_political / government', 'job_rather not say', 'job_retired',\n",
       "       'job_sales / marketing / biz dev',\n",
       "       'job_science / tech / engineering', 'job_student',\n",
       "       'job_transportation', 'job_unemployed', 'job_nan',\n",
       "       'location_arizona', 'location_california', 'location_canada',\n",
       "       'location_colorado', 'location_connecticut',\n",
       "       'location_district of columbia', 'location_florida',\n",
       "       'location_georgia', 'location_germany', 'location_hawaii',\n",
       "       'location_idaho', 'location_illinois', 'location_ireland',\n",
       "       'location_louisiana', 'location_massachusetts', 'location_mexico',\n",
       "       'location_michigan', 'location_minnesota', 'location_mississippi',\n",
       "       'location_missouri', 'location_montana', 'location_netherlands',\n",
       "       'location_nevada', 'location_new jersey', 'location_new york',\n",
       "       'location_north carolina', 'location_ohio', 'location_oregon',\n",
       "       'location_pennsylvania', 'location_rhode island', 'location_spain',\n",
       "       'location_switzerland', 'location_tennessee', 'location_texas',\n",
       "       'location_united kingdom', 'location_utah', 'location_vietnam',\n",
       "       'location_virginia', 'location_washington',\n",
       "       'location_west virginia', 'location_wisconsin',\n",
       "       \"offspring_doesn't have kids\",\n",
       "       \"offspring_doesn't have kids, and doesn't want any\",\n",
       "       \"offspring_doesn't have kids, but might want them\",\n",
       "       \"offspring_doesn't have kids, but wants them\",\n",
       "       \"offspring_doesn't want kids\", 'offspring_has kids',\n",
       "       'offspring_has kids, and might want more',\n",
       "       'offspring_has kids, and wants more',\n",
       "       \"offspring_has kids, but doesn't want more\",\n",
       "       'offspring_might want kids', 'offspring_wants kids',\n",
       "       'offspring_nan', 'religion_agnosticism',\n",
       "       'religion_agnosticism and laughing about it',\n",
       "       'religion_agnosticism and somewhat serious about it',\n",
       "       'religion_agnosticism and very serious about it',\n",
       "       'religion_agnosticism but not too serious about it',\n",
       "       'religion_atheism', 'religion_atheism and laughing about it',\n",
       "       'religion_atheism and somewhat serious about it',\n",
       "       'religion_atheism and very serious about it',\n",
       "       'religion_atheism but not too serious about it',\n",
       "       'religion_buddhism', 'religion_buddhism and laughing about it',\n",
       "       'religion_buddhism and somewhat serious about it',\n",
       "       'religion_buddhism and very serious about it',\n",
       "       'religion_buddhism but not too serious about it',\n",
       "       'religion_catholicism',\n",
       "       'religion_catholicism and laughing about it',\n",
       "       'religion_catholicism and somewhat serious about it',\n",
       "       'religion_catholicism and very serious about it',\n",
       "       'religion_catholicism but not too serious about it',\n",
       "       'religion_christianity',\n",
       "       'religion_christianity and laughing about it',\n",
       "       'religion_christianity and somewhat serious about it',\n",
       "       'religion_christianity and very serious about it',\n",
       "       'religion_christianity but not too serious about it',\n",
       "       'religion_hinduism', 'religion_hinduism and laughing about it',\n",
       "       'religion_hinduism and somewhat serious about it',\n",
       "       'religion_hinduism and very serious about it',\n",
       "       'religion_hinduism but not too serious about it', 'religion_islam',\n",
       "       'religion_islam and laughing about it',\n",
       "       'religion_islam and somewhat serious about it',\n",
       "       'religion_islam and very serious about it',\n",
       "       'religion_islam but not too serious about it', 'religion_judaism',\n",
       "       'religion_judaism and laughing about it',\n",
       "       'religion_judaism and somewhat serious about it',\n",
       "       'religion_judaism and very serious about it',\n",
       "       'religion_judaism but not too serious about it', 'religion_other',\n",
       "       'religion_other and laughing about it',\n",
       "       'religion_other and somewhat serious about it',\n",
       "       'religion_other and very serious about it',\n",
       "       'religion_other but not too serious about it', 'religion_nan',\n",
       "       'sex_f', 'sex_m', 'smokes_no', 'smokes_sometimes',\n",
       "       'smokes_trying to quit', 'smokes_when drinking', 'smokes_yes',\n",
       "       'smokes_nan', 'status_available', 'status_married',\n",
       "       'status_seeing someone', 'status_single', 'status_unknown'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_feature_names = ct.named_transformers_['encoder'].get_feature_names_out()\n",
    "ohe_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['single', 'available', 'seeing someone', 'married', 'unknown'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = pd.DataFrame({\n",
    "    'age': [24],\n",
    "    'body_type': ['fit'],\n",
    "    'diet': ['anything'],\n",
    "    'drinks': ['socially'],\n",
    "    'drugs':['never'],\n",
    "    'education':['college/university'],\n",
    "    'ethnicity':['asian, white'],\n",
    "    'income':[1000000],\n",
    "    'job':['banking / financial / real estate'],\n",
    "    'location':['california'],\n",
    "    'offspring':[\"doesn't have kids, but might want them\"],\n",
    "    'religion':['agnosticism but not too serious about it'],\n",
    "    'sex':['m'],\n",
    "    'smokes':['no'],\n",
    "    'status':['seeing someone']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the test vector using the ColumnTransformer\n",
    "test_vector_transformed = ct.transform(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['straight'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(test_vector_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "straight    51606\n",
      "gay          5573\n",
      "bisexual     2767\n",
      "Name: orientation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['orientation'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
